{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d09c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DEVICE = cuda\n",
      "==================================================\n",
      "Image: 256x512 | Batch: 8 | Epochs: 20\n",
      "Target Classes: 20\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# CELL 1: SETUP & IMPORTS\n",
    "############################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"=\" * 50)\n",
    "print(f\"DEVICE = {DEVICE}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "IMG_H, IMG_W = 256, 512\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "print(f\"Image: {IMG_H}x{IMG_W} | Batch: {BATCH_SIZE} | Epochs: {EPOCHS}\")\n",
    "print(f\"Target Classes: {NUM_CLASSES}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a63c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# CELL 2: DATA PREPROCESSING\n",
    "############################################################\n",
    "print(f\"Binning grayscale values (0-255) into {NUM_CLASSES} classes...\")\n",
    "\n",
    "def value_to_class(value):\n",
    "    \"\"\"Map grayscale value (0-255) to class (0-19)\"\"\"\n",
    "    if value == 255:\n",
    "        return NUM_CLASSES  # ignore\n",
    "    return min(int(value * NUM_CLASSES / 256), NUM_CLASSES - 1)\n",
    "\n",
    "print(f\"âœ“ Mapping created: 0-255 â†’ 0-{NUM_CLASSES-1}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c7651",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# CELL 3: DATASET CLASS\n",
    "############################################################\n",
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, augment=False):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.files = sorted(os.listdir(img_dir))\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.files[idx]\n",
    "        \n",
    "        img = np.array(Image.open(os.path.join(self.img_dir, name)).convert(\"RGB\"))\n",
    "        mask = cv2.imread(os.path.join(self.mask_dir, name), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        img = cv2.resize(img, (IMG_W, IMG_H))\n",
    "        mask = cv2.resize(mask, (IMG_W, IMG_H), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        if self.augment and np.random.rand() > 0.5:\n",
    "            img = np.fliplr(img).copy()\n",
    "            mask = np.fliplr(mask).copy()\n",
    "        \n",
    "        mask_mapped = np.vectorize(value_to_class)(mask).astype(np.int64)\n",
    "        \n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "        \n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "        mask_mapped = torch.from_numpy(mask_mapped).long()\n",
    "        \n",
    "        return img, mask_mapped\n",
    "\n",
    "print(\"âœ“ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ba8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# CELL 4: CREATE DATALOADERS\n",
    "############################################################\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = CityscapesDataset(\"train/img\", \"train/label\", augment=True)\n",
    "val_dataset = CityscapesDataset(\"val/img\", \"val/label\", augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)}\")\n",
    "\n",
    "# Test data loading\n",
    "print(\"\\nTesting data...\")\n",
    "test_img, test_mask = train_dataset[0]\n",
    "print(f\"Image: {test_img.shape}\")\n",
    "print(f\"Mask: {test_mask.shape}\")\n",
    "unique = torch.unique(test_mask)\n",
    "print(f\"Unique classes: {unique.tolist()}\")\n",
    "valid_pixels = (test_mask < NUM_CLASSES).sum().item()\n",
    "total_pixels = test_mask.numel()\n",
    "print(f\"Valid pixels: {valid_pixels} / {total_pixels} ({100*valid_pixels/total_pixels:.1f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# CELL 5: MODEL ARCHITECTURE WITH SELF-ATTENTION\n",
    "############################################################\n",
    "\n",
    "# Self-Attention Module \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value = nn.Conv2d(channels, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        \n",
    "        # Query, Key, Value\n",
    "        Q = self.query(x).view(B, -1, H * W).permute(0, 2, 1)  # (B, HW, C')\n",
    "        K = self.key(x).view(B, -1, H * W)                      # (B, C', HW)\n",
    "        V = self.value(x).view(B, -1, H * W)                    # (B, C, HW)\n",
    "        \n",
    "        # Attention scores\n",
    "        attention = torch.bmm(Q, K)  # (B, HW, HW)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.bmm(V, attention.permute(0, 2, 1))\n",
    "        out = out.view(B, C, H, W)\n",
    "        \n",
    "        # Residual connection with learnable weight\n",
    "        out = self.gamma * out + x\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Convolutional Block\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# U-Net with Self-Attention\n",
    "class FastUNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: ResNet34 backbone\n",
    "        resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        self.enc1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)\n",
    "        self.enc2 = nn.Sequential(resnet.maxpool, resnet.layer1)\n",
    "        self.enc3 = resnet.layer2\n",
    "        self.enc4 = resnet.layer3\n",
    "        self.enc5 = resnet.layer4\n",
    "        \n",
    "        # âœ… Self-Attention Modules \n",
    "        self.att5 = SelfAttention(512)\n",
    "        self.att4 = SelfAttention(256)\n",
    "        self.att3 = SelfAttention(128)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up5 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec5 = ConvBlock(512, 256)\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec4 = ConvBlock(256, 128)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(128, 64)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(128, 64)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(64, 64)\n",
    "        \n",
    "        self.final = nn.Conv2d(64, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        \n",
    "        # âœ… Apply Self-Attention\n",
    "        e5 = self.att5(e5)\n",
    "        e4 = self.att4(e4)\n",
    "        e3 = self.att3(e3)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d5 = self.up5(e5)\n",
    "        d5 = torch.cat([d5, e4], dim=1)\n",
    "        d5 = self.dec5(d5)\n",
    "        \n",
    "        d4 = self.up4(d5)\n",
    "        d4 = torch.cat([d4, e3], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        \n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, e2], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e1], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        return self.final(d1)\n",
    "\n",
    "print(\"âœ“ Model architecture with Self-Attention defined\")\n",
    "print(\"  - Self-Attention modules: 3 (manually implemented)\")\n",
    "print(\"  - Encoder: ResNet34\")\n",
    "print(\"  - Decoder: U-Net style with skip connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# CELL 6: BUILD MODEL\n",
    "############################################################\n",
    "print(\"Building model...\")\n",
    "model = FastUNet(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "print(f\"âœ“ Model Ready ({NUM_CLASSES} classes)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# CELL 7: TRAINING SETUP\n",
    "############################################################\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=NUM_CLASSES)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "def evaluate(model, loader, max_batches=100):\n",
    "    model.eval()\n",
    "    correct, pixels = 0, 0\n",
    "    ious = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, masks) in enumerate(loader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "            preds = torch.argmax(model(imgs), dim=1)\n",
    "            \n",
    "            valid = (masks < NUM_CLASSES)\n",
    "            correct += (preds[valid] == masks[valid]).sum().item()\n",
    "            pixels += valid.sum().item()\n",
    "            \n",
    "            for c in range(NUM_CLASSES):\n",
    "                inter = ((preds == c) & (masks == c)).sum().item()\n",
    "                union = ((preds == c) | (masks == c)).sum().item()\n",
    "                if union > 0:\n",
    "                    ious.append(inter / union)\n",
    "    \n",
    "    return correct/pixels if pixels > 0 else 0, np.mean(ious) if ious else 0\n",
    "\n",
    "print(\"âœ“ Training setup ready\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f559ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# CELL 8: TRAINING LOOP\n",
    "############################################################\n",
    "print(\"ðŸ”¥ TRAINING STARTED ðŸ”¥\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_miou = 0\n",
    "losses, accs, mious = [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for imgs, masks in pbar:\n",
    "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, masks)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0 or epoch == 0 or epoch >= EPOCHS - 2:\n",
    "        print(\"\\nEvaluating...\")\n",
    "        acc, miou = evaluate(model, val_loader)\n",
    "        accs.append(acc)\n",
    "        mious.append(miou)\n",
    "        \n",
    "        if miou > best_miou:\n",
    "            best_miou = miou\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"ðŸ’¾ Saved! mIoU={miou*100:.1f}%\")\n",
    "    else:\n",
    "        accs.append(accs[-1] if accs else 0)\n",
    "        mious.append(mious[-1] if mious else 0)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Loss={avg_loss:.3f} | Acc={accs[-1]*100:.1f}% | mIoU={mious[-1]*100:.1f}% | Best={best_miou*100:.1f}%\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# CELL 9: FINAL EVALUATION\n",
    "############################################################\n",
    "print(\"ðŸ“Š FINAL EVALUATION\")\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "final_acc, final_miou = evaluate(model, val_loader, max_batches=250)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"âœ… Pixel Accuracy = {final_acc*100:.2f}%\")\n",
    "print(f\"âœ… mIoU           = {final_miou*100:.2f}%\")\n",
    "print(f\"âœ… Classes        = {NUM_CLASSES}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# CELL 10: SAVE MODEL\n",
    "############################################################\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'pixel_acc': final_acc,\n",
    "    'miou': final_miou\n",
    "}, \"cityscapes_checkpoint.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), \"cityscapes_model_weights.pth\")\n",
    "\n",
    "print(\"âœ“ Model saved!\")\n",
    "print(\"  - best_model.pth\")\n",
    "print(\"  - cityscapes_checkpoint.pth\")\n",
    "print(\"  - cityscapes_model_weights.pth\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06146f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# CELL 11: PLOT RESULTS\n",
    "############################################################\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot([a*100 for a in accs])\n",
    "plt.title(\"Pixel Accuracy (%)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot([m*100 for m in mious])\n",
    "plt.title(\"mIoU (%)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"mIoU\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_results.png\", dpi=120)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Training curves saved: training_results.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
